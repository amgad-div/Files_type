{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Files type, Readind speed and size"
      ],
      "metadata": {
        "id": "th71SMduKedh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install fastavro"
      ],
      "metadata": {
        "id": "CeaDVqqAMPMq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JldsK_VVG6QR",
        "outputId": "54c33fcc-edee-4902-c9eb-aff7e2b9b051"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting fastavro\n",
            "  Downloading fastavro-1.11.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.7 kB)\n",
            "Downloading fastavro-1.11.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m37.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: fastavro\n",
            "Successfully installed fastavro-1.11.1\n",
            "Generating large dataset...\n",
            "Dataset generated with 1000000 rows and 5 columns.\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import time\n",
        "import pickle\n",
        "import fastavro\n",
        "import pyarrow.orc as orc\n",
        "import pyarrow.feather as feather\n",
        "\n",
        "\n",
        "# Generating large dataset (1000000)\n",
        "print(\"Generating large dataset...\")\n",
        "num_rows = 1000000\n",
        "data = {\n",
        "    'id': range(num_rows),\n",
        "    'name': [f'User_{i}' for i in range(num_rows)],\n",
        "    'value_a': [float(i * 1.5) for i in range(num_rows)],\n",
        "    'category': [f'Category_{i % 10}' for i in range(num_rows)],\n",
        "    'description': [f'This is a long description for item {i}' for i in range(num_rows)]\n",
        "}\n",
        "df = pd.DataFrame(data)\n",
        "print(f\"Dataset generated with {len(df)} rows and {len(df.columns)} columns.\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(df.head(5))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kkJd6lOVHv23",
        "outputId": "f1d31477-d423-47e0-d830-33db6f5442af"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   id    name  value_a    category                            description\n",
            "0   0  User_0      0.0  Category_0  This is a long description for item 0\n",
            "1   1  User_1      1.5  Category_1  This is a long description for item 1\n",
            "2   2  User_2      3.0  Category_2  This is a long description for item 2\n",
            "3   3  User_3      4.5  Category_3  This is a long description for item 3\n",
            "4   4  User_4      6.0  Category_4  This is a long description for item 4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Testing Write Speed and File Size"
      ],
      "metadata": {
        "id": "VaVOozJ5I6bd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Make path for data\n",
        "output_dir = 'data_speed_test'\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "csv_file = os.path.join(output_dir, 'large_data.csv')\n",
        "json_file = os.path.join(output_dir, 'large_data.json')\n",
        "parquet_file = os.path.join(output_dir, 'large_data.parquet')\n",
        "pickle_file = os.path.join(output_dir, 'large_data.pkl')\n",
        "avro_file = os.path.join(output_dir, 'large_data.avro')\n",
        "orc_file = os.path.join(output_dir, 'large_data.orc')\n",
        "feather_file = os.path.join(output_dir, 'large_data.feather')\n",
        "\n",
        "print(\"\\n--- Testing Write Speed and File Size ---\")\n",
        "\n",
        "# Writing in CSV file\n",
        "start_time = time.time()\n",
        "df.to_csv(csv_file, index=False)\n",
        "end_time = time.time()\n",
        "print(f\"CSV Write Time: {end_time - start_time:.4f} seconds\")\n",
        "print(f\"CSV File Size: {os.path.getsize(csv_file) / (1024 * 1024):.2f} MB\")\n",
        "\n",
        "print(\"- - -\"*10)\n",
        "# Writing in JSON file\n",
        "start_time = time.time()\n",
        "df.to_json(json_file, orient='records', lines=True)  # lines=True makes it JSONL (JSON Lines)\n",
        "end_time = time.time()\n",
        "print(f\"JSON Write Time: {end_time - start_time:.4f} seconds\")\n",
        "print(f\"JSON File Size: {os.path.getsize(json_file) / (1024 * 1024):.2f} MB\")\n",
        "\n",
        "print(\"- - -\"*10)\n",
        "# Writing in Parquet file\n",
        "start_time = time.time()\n",
        "df.to_parquet(parquet_file, engine='pyarrow', index=False)\n",
        "end_time = time.time()\n",
        "print(f\"Parquet Write Time: {end_time - start_time:.4f} seconds\")\n",
        "print(f\"Parquet File Size: {os.path.getsize(parquet_file) / (1024 * 1024):.2f} MB\")\n",
        "\n",
        "print(\"- - -\"*10)\n",
        "# Writing in ORC file\n",
        "start_time = time.time()\n",
        "df.to_orc(orc_file, engine='pyarrow')\n",
        "end_time = time.time()\n",
        "print(f\"ORC Write Time: {end_time - start_time:.4f} seconds\")\n",
        "print(f\"ORC File Size: {os.path.getsize(orc_file) / (1024 * 1024):.2f} MB\")\n",
        "\n",
        "print(\"- - -\"*10)\n",
        "# Writing in Feather file\n",
        "start_time = time.time()\n",
        "df.to_feather(feather_file)\n",
        "end_time = time.time()\n",
        "print(f\"Feather Write Time: {end_time - start_time:.4f} seconds\")\n",
        "print(f\"Feather File Size: {os.path.getsize(feather_file) / (1024 * 1024):.2f} MB\")\n",
        "\n",
        "print(\"- - -\"*10)\n",
        "# Writing in Pickle file\n",
        "start_time = time.time()\n",
        "with open(pickle_file, 'wb') as f:\n",
        "    pickle.dump(df, f)\n",
        "end_time = time.time()\n",
        "print(f\"Pickle Write Time: {end_time - start_time:.4f} seconds\")\n",
        "print(f\"Pickle File Size: {os.path.getsize(pickle_file) / (1024 * 1024):.2f} MB\")\n",
        "\n",
        "print(\"- - -\"*10)\n",
        "# Writing in Avro file\n",
        "schema = {\n",
        "    \"type\": \"record\",\n",
        "    \"name\": \"DataFrame\",\n",
        "    \"fields\": [{\"name\": col, \"type\": \"string\" if df[col].dtype == 'object' else \"int\"} for col in df.columns]\n",
        "}\n",
        "\n",
        "start_time = time.time()\n",
        "with open(avro_file, 'wb') as f:\n",
        "    writer = fastavro.writer(f, schema, df.to_dict(orient=\"records\"))\n",
        "end_time = time.time()\n",
        "print(f\"Avro Write Time: {end_time - start_time:.4f} seconds\")\n",
        "print(f\"Avro File Size: {os.path.getsize(avro_file) / (1024 * 1024):.2f} MB\")\n",
        "\n",
        "print(\"\\n--- Testing Read Speed ---\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uNN9VEh8HiGR",
        "outputId": "220bb897-7b22-435e-d014-fe6000f1b305"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Testing Write Speed and File Size ---\n",
            "CSV Write Time: 5.4887 seconds\n",
            "CSV File Size: 78.13 MB\n",
            "- - -- - -- - -- - -- - -- - -- - -- - -- - -- - -\n",
            "JSON Write Time: 2.8544 seconds\n",
            "JSON File Size: 130.58 MB\n",
            "- - -- - -- - -- - -- - -- - -- - -- - -- - -- - -\n",
            "Parquet Write Time: 0.7509 seconds\n",
            "Parquet File Size: 18.24 MB\n",
            "- - -- - -- - -- - -- - -- - -- - -- - -- - -- - -\n",
            "ORC Write Time: 0.9696 seconds\n",
            "ORC File Size: 67.56 MB\n",
            "- - -- - -- - -- - -- - -- - -- - -- - -- - -- - -\n",
            "Feather Write Time: 0.4326 seconds\n",
            "Feather File Size: 28.41 MB\n",
            "- - -- - -- - -- - -- - -- - -- - -- - -- - -- - -\n",
            "Pickle Write Time: 1.7395 seconds\n",
            "Pickle File Size: 83.73 MB\n",
            "- - -- - -- - -- - -- - -- - -- - -- - -- - -- - -\n",
            "Avro Write Time: 9.3115 seconds\n",
            "Avro File Size: 68.82 MB\n",
            "\n",
            "--- Testing Read Speed ---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Testing Reading Speed"
      ],
      "metadata": {
        "id": "2rw0Zs6xKaLp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Reading from csv file\n",
        "start_time = time.time()\n",
        "df_csv = pd.read_csv(csv_file)\n",
        "end_time = time.time()\n",
        "print(f\"CSV Read Time: {end_time - start_time:.4f} seconds\")\n",
        "\n",
        "# Reading from JSON file\n",
        "start_time = time.time()\n",
        "df_json = pd.read_json(json_file, orient='records', lines=True)\n",
        "end_time = time.time()\n",
        "print(f\"JSON Read Time: {end_time - start_time:.4f} seconds\")\n",
        "\n",
        "# Reading from Parquet file (all columns)\n",
        "start_time = time.time()\n",
        "df_parquet = pd.read_parquet(parquet_file, engine='pyarrow')\n",
        "end_time = time.time()\n",
        "print(f\"Parquet Read All Columns Time: {end_time - start_time:.4f} seconds\")\n",
        "\n",
        "# Reading from ORC file\n",
        "start_time = time.time()\n",
        "df_orc = pd.read_orc(orc_file)  # بدون تحديد محرك \"engine\"\n",
        "end_time = time.time()\n",
        "print(f\"ORC Read Time: {end_time - start_time:.4f} seconds\")\n",
        "\n",
        "# Reading from Feather file\n",
        "start_time = time.time()\n",
        "df_feather = pd.read_feather(feather_file)\n",
        "end_time = time.time()\n",
        "print(f\"Feather Read Time: {end_time - start_time:.4f} seconds\")\n",
        "\n",
        "# Reading from Pickle file\n",
        "start_time = time.time()\n",
        "with open(pickle_file, 'rb') as f:\n",
        "    df_pickle = pickle.load(f)\n",
        "end_time = time.time()\n",
        "print(f\"Pickle Read Time: {end_time - start_time:.4f} seconds\")\n",
        "\n",
        "# Reading from Avro file\n",
        "start_time = time.time()\n",
        "with open(avro_file, 'rb') as f:\n",
        "    reader = fastavro.reader(f)\n",
        "    df_avro = pd.DataFrame(list(reader))\n",
        "end_time = time.time()\n",
        "print(f\"Avro Read Time: {end_time - start_time:.4f} seconds\")\n",
        "\n",
        "# Reading from Parquet file (only one row)\n",
        "print(\"\\n--- Testing Parquet Read Speed ---\")\n",
        "start_time = time.time()\n",
        "df_parquet_col = pd.read_parquet(parquet_file, engine='pyarrow', columns=['value_a'])\n",
        "end_time = time.time()\n",
        "print(f\"Parquet Read 'value_a' Column Only Time: {end_time - start_time:.4f} seconds\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xt_BgCyTQTQ9",
        "outputId": "7a411611-aa96-4c6d-ef24-36003d88650a"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CSV Read Time: 2.8576 seconds\n",
            "JSON Read Time: 9.3556 seconds\n",
            "Parquet Read All Columns Time: 1.1887 seconds\n",
            "ORC Read Time: 0.9816 seconds\n",
            "Feather Read Time: 0.8949 seconds\n",
            "Pickle Read Time: 0.4047 seconds\n",
            "Avro Read Time: 4.7201 seconds\n",
            "\n",
            "--- Testing Parquet Read Speed ---\n",
            "Parquet Read 'value_a' Column Only Time: 0.0539 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZVQrfHkhQT4e"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}